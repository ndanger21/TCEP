constants {
  transition-testing = false # if this is enabled, system will cycle through all available placements
  default-load = 1.0
  retry-timeout = 4 // s time to wait before re-sending a msg
  default-retries = 5 // number of attempts to send a message (can be used for TCEPUtils.trySendWithReply()
  default-request-timeout = 15 // s, used for load/bandwidth/etc requests
  coordinates-refresh-interval = 5
  coordinate-request-timeout = 15
  coordinates-error-threshold = 4.0
  transition-execution-mode = 1 # Can be 0 for "Sequential" or 1 for "Concurrent"
  event-interval-microseconds = 1000000 # ~1 events/second
  default-data-rate = 30.0 # Mbit/s Default data rate to use when there is no measurement yet
  simulation-time = 2
  gui-endpoint = "http://gui:3000"
  prediction-endpoint = "http://predictionendpoint:9091/"
  number-of-speed-publisher-nodes = 4 # number of publisher nodes that are SpeedPublishers
  number-of-road-sections = 4 # must be even; number of sections the road is divided into, i.e. points at which mobility is simulated by switching connections of publisher containers; affects "granularity" of mobility simulation
  isLocalSwarm = false
  mininet-simulation = false
  mininet-link-bandwidth = 100.0 // Mbit/s; link bandwidth set in mininet simulation
  global-initialization-timeout = 60000 // ms
  base-port = 2500
  prediction-lag-count = 3
  predictionHorizonLength = 1000 # [ms] how far into the future we want to make predictions
  predictionHorizonStepCount = 1 # points at which to make predictions within that horizon; 0 -> t_now, 1 -> t_now and t_now + horizonLength
  mobility-simulation {
    //enabled = true
    //delay = 5 // s, delay before publishers start sending events in a mobility simulation (to avoid running out of mobility trace data since only ~25 minutes available; simulation start is delayed by bandwidth measurements)
    road-section-length = 2500
  }
  // the MAPEK-Implementation controlling transitions to use
  // CONTRAST: make transitions according to performance predictions based on context information
  //    - a context-feature model (CFM) captures information about the context and system state
  //    - predictions are based on the current CFM configuration, which serves as input for a  Multiple Linear Regression model (learned with SPLConqueror)
  //    -  when choosing which placement algorithm to transit to, predictions for the respective metric are taken into account if QoS requirements exist
  //    - triggers for execution: QoS requirement change, context changes (checked periodically)
  // requirementBased: make transition based on QoS requirements (see benchmark configuration section)
  //    - triggers for execution: QoS requirement changes, nodes being shut down or started
  mapek {
    exchangeable-model.use-online-model-weighting = true
    exchangeable-model.online-model-warmup-phase = 30 # [samples] min samples seen per operator before making first predictions with online models
    exchangeable-model.optimization-target = "latency"
    type = "ExchangeablePerformanceModel"
    availableTypes = ["requirementBased", "lightweight", "CONTRAST", "LearnOn", "ExchangeablePerformanceModel"]
    sampling-interval = 5000 // how often the learning model receives data in [ms]
    transition-cooldown = 30 // s; time that must have passed before allowing another transition -> avoid oscillating transitions
    improvement-threshold = 0.0 // percentage of predicted metric improvement over current value that is necessary to allow a transition -> take predicition inaccuracy into account when making transition decision
    blacklisted-algorithms = [] // algorithms that will never be selected by CONTRAST
    transitions-enabled = true // TODO disable transitions for SPLC learning data collection
    enable-distributed-transition-debugging = false // enable (pub/sub) debug logging for transitions (causes additional messaging overhead)
    only-suitable = false // use only OP mechanisms, which optimize for the given QoS requirements. Not necessary anymore
    lightweight-decay = 0.5 // Weighting for old vs new value. [0 , 0.5]
    learning-model = "rl" // Specifies which learning model is used in LearnOn ["learnOn", "rl", "lightweight"]
    latency-madrid-pim-path = "/performanceModels/madridLatencyModel.log"
    load-madrid-pim-path = "/performanceModels/madridLoadModel.log"
    latency-linear-road-pim-path = "/performanceModels/linearRoadLatencyModel.log"
    load-linear-road-pim-path = "/performanceModels/linearRoadLoadModel.log"
    latency-yahoo-pim-path = "/performanceModels/yahooLatencyModel.log"
    load-yahoo-pim-path = "/performanceModels/yahooLoadModel.log"
    latency-combined-pim-path = "/performanceModels/combinedLatencyModel.log"
    load-combined-pim-path = "/performanceModels/combinedLoadModel.log"

    latency-madrid-log-pim-path = "/performanceModels/madridLogLatencyModel.log"
    load-madrid-log-pim-path = "/performanceModels/madridLogLoadModel.log"
    latency-linear-road-log-pim-path = "/performanceModels/linearRoadLogLatencyModel.log"
    load-linear-road-log-pim-path = "/performanceModels/linearRoadLogLoadModel.log"
    latency-yahoo-log-pim-path = "/performanceModels/yahooLogLatencyModel.log"
    load-yahoo-log-pim-path = "/performanceModels/yahooLogLoadModel.log"
    latency-combined-log-pim-path = "/performanceModels/combinedLogLatencyModel.log"
    load-combined-log-pim-path = "/performanceModels/combinedLogLoadModel.log"

    naive-latency-madrid-log-pim-path = "/performanceModels/naiveMadridLogLatencyModel.log"
    naive-load-madrid-log-pim-path = "/performanceModels/naiveMadridLogLoadModel.log"
    naive-latency-linear-road-log-pim-path = "/performanceModels/naiveLinearLogLatencyModel.log"
    naive-load-linear-road-log-pim-path = "/performanceModels/naiveLinearLogLoadModel.log"
    naive-latency-yahoo-log-pim-path = "/performanceModels/naiveYahooLogLatencyModel.log"
    naive-load-yahoo-log-pim-path = "/performanceModels/naiveYahooLogLoadModel.log"


  }

  madrid-traces {
    minimum-number-of-nodes = 17 // set by manifest_to_config.py (mininet: set manually to same value as mininet_wired_tree.py)
    number-of-publishers = 8 //  ""
    number-of-road-sections = 4
    road-section-length = 5000
  }

  linear-road {
    number-of-publishers = 6 //  ""
    minimum-number-of-nodes = 16 // set by manifest_to_config.py (mininet: set manually to same value as mininet_wired_tree.py)
  }

  yahoo {
    number-of-publishers = 8
    minimum-number-of-nodes = 16//13 increased by two to increase throughput
  }

  learnon {
    power = true // used as a power button. false -> not learning is equivalent to using CONTRAST
    opie-storage = 0 // Number of samples to store in the sliding window and is used for training
    opie-loss-type = "linear" // Kind of loss. Possible: "linear", "square", "exp"
    prediction = "mean" // kind of predicition to deploy by LearnOn. Currently only mean is supported
    offline-weight = 0.5 //how much weight to put on the offline model with mean prediction
    log-prediction = true //predict log of metrics
    naive-models = true // use the naive model instead of the SPLC one
    full-model-weight = true // weight according to the complete model rather than the last added hypothesis
    hypothesis-weighting = false // true -> weight according to AIDAN, false -> deploy equal weight for each added hypothesis
    weight-decay = 1.0 // is multiplied with each hypotheses weight after adding a new one. Shrinks the weight over time
  }

  modelrl {
    rl-decay = 0.9 //the decay parameter used in the bellman equation
    update-every = 5//s after an Q-Table update to start update again
    init-phase = true //No init phase should make no sence. So leave it on
    zero-start = false // Initialize the state-transition probabilities with zeros or ones
  }

  placement {
    placement-request-timeout = 30 // s
    physical-placement-node-overload-threshold = 3.0 // unix cpu load level at which a node is considered overloaded
    physical-placement-nearest-neighbours = 3
    relaxation-initial-step-size = 0.1
    relaxation-step-adjustment-enabled = true // does not work well (overflows, oscillations) without stepsize adjustment
    max-single-operator-iterations = 200
    relaxation-initial-iterations = 30
    update-interval = 60 // seconds after which node re-evaluates its own operators placement
    update { // enable or disable placement update functionality
      relaxation = false
      rizou = false
      starks = false
    }
  }

  query = ["Stream", "Filter", "Disjunction", "Join", "SelfJoin"]
  //query = ["Stream", "Filter", "Conjunction", "Disjunction", "Join", "SelfJoin", "Accident Detection"]
  //query = ["Conjunction"]

}

benchmark {
  general {
    algorithms = [
      "Relaxation",
      "Rizou",
      "ProducerConsumer",
      "Random",
      "GlobalOptimalBDP",
      "MDCEP"
    ]
  }

  Relaxation {
    optimizationCriteria = ["latency", "machineLoad"]
    constraints = ["LowChurnRate"]
    class = "tcep.placement.sbon.PietzuchAlgorithm$"
    score = 100
  }

  MDCEP {
    optimizationCriteria = ["messageHops", "machineLoad"]
    constraints = ["LowChurnRate"]
    class = "tcep.placement.manets.StarksAlgorithm$"
    score = 100
  }

  Rizou {
    optimizationCriteria = ["latency", "machineLoad"]
    constraints = ["LowChurnRate"]
    class = "tcep.placement.mop.RizouAlgorithm$"
    score = 100 # Relaxation has the same criteria -> change value here to make both selectable in RequirementBasedMAPEK via requirement change
  }

  ProducerConsumer {
    optimizationCriteria = ["messageHops"]
    constraints = ["HighChurnRate"]
    class = "tcep.placement.MobilityTolerantAlgorithm$"
    score = 100
  }

  Random {
    optimizationCriteria = []
    constraints = ["LowChurnRate"]
    class = "tcep.placement.RandomAlgorithm$"
    score = 200
  }

  GlobalOptimalBDP {
    optimizationCriteria = ["latency", "messageHops"]
    constraints = ["LowChurnRate"]
    class = "tcep.placement.GlobalOptimalBDPAlgorithm$"
    score = 100
  }
}

# define prio mailbox to be passed in Props of new actors that handle messages of differing priority
prio-mailbox {
  mailbox-type = "tcep.akkamailbox.TCEPPriorityMailbox"
}

# dedicated dispatcher to be used by all disk- or network I/O-bound tasks and futures
# this alias points to the default blocking-io-dispatcher, which is shared for some akka-related tasks. Use and configure custom dispatcher below instead if system behaves erratically under load due to thread starvation
#blocking-io-dispatcher = "akka.actor.default-blocking-io-dispatcher"
blocking-io-dispatcher {
  type = Dispatcher
  executor = "fork-join-executor"
  fork-join-executor {
    parallelism-min = 4
     parallelism-factor = 1.0
     # Note that the parallelism-max does not set the upper bound on the total number of threads allocated by the ForkJoinPool.
     # It is a setting specifically talking about the number of hot threads the pool keep running in order to reduce the latency of handling a new incoming task
     parallelism-max = 12
  }
  # Throughput defines the maximum number of messages to be
  # processed per actor before the thread jumps to the next actor.
  # Set to 1 for as fair as possible.
  throughput = 1
}

transition-dispatcher {
type = Dispatcher
  executor = "fork-join-executor"
  fork-join-executor {
    parallelism-min = 4
     parallelism-factor = 1.0
     # Note that the parallelism-max does not set the upper bound on the total number of threads allocated by the ForkJoinPool.
     # It is a setting specifically talking about the number of hot threads the pool keep running in order to reduce the latency of handling a new incoming task
     parallelism-max = 8
  }
  throughput = 1
}

prediction-dispatcher {
type = Dispatcher
  executor = "fork-join-executor"
  fork-join-executor {
    parallelism-min = 4
     parallelism-factor = 1.0
     # Note that the parallelism-max does not set the upper bound on the total number of threads allocated by the ForkJoinPool.
     # It is a setting specifically talking about the number of hot threads the pool keep running in order to reduce the latency of handling a new incoming task
     parallelism-max = 8
  }
  throughput = 5
}

akka {
    # - for http requests to prediction server -
    # The maximum number of parallel connections that a connection pool to a
    # single host endpoint is allowed to establish. Must be greater than zero.
  http.host-connection-pool.max-connections = 50 # default 4; avoid sending predict + train requests in parallel
    # The maximum number of open requests accepted into the pool across all
    # materializations of any of its client flows.
    # Protects against (accidentally) overloading a single pool with too many client flow materializations.
    # Note that with N concurrent materializations the max number of open request in the pool
    # will never exceed N * max-connections * pipelining-limit.
    # Must be a power of 2 and > 0!
  http.host-connection-pool.max-open-requests = 512 # default 32; during placement performance prediction, we run 1 request per operator per placement algorithm i.e. 46 * 6 = 276
  # Loggers to register at boot time (akka.event.Logging$DefaultLogger logs
  # to STDOUT)
  loggers = ["akka.event.slf4j.Slf4jLogger"]
  logger-startup-timeout = 30s
  jvm-exit-on-fatal-error = false

  # Log level used by the configured loggers (see "loggers") as soon
  # as they have been started; before that, see "stdout-loglevel"
  # Options: OFF, ERROR, WARNING, INFO, DEBUG
  loglevel = "DEBUG"

  # Log level for the very basic logger activated during ActorSystem startup.
  # This logger prints the log messages to stdout (System.out).
  # Options: OFF, ERROR, WARNING, INFO, DEBUG
  stdout-loglevel = "DEBUG"

  # Filter of log events that is usved by the LoggingAdapter before
  # publishing log events to the eventStream.
  # logging-filter = "akka.event.slf4j.Slf4jLoggingFilter"

  log-config-on-start = off
  log-dead-letters = on

  actor {
    debug.receive = off
    debug.unhandled = on

    provider = "cluster"
    default-dispatcher {
       # This will be used if you have set "executor = "fork-join-executor""
       # Underlying thread pool implementation is java.util.concurrent.ForkJoinPool
       fork-join-executor {
         # Min number of threads to cap factor-based parallelism number to
         parallelism-min = 4

         # The parallelism factor is used to determine thread pool size using the
         # following formula: ceil(available processors * factor). Resulting size
         # is then bounded by the parallelism-min and parallelism-max values.
         # this determines how many threads are kept in waiting (available unless doing work) in the pool depending on the number of available cores on the cpu
         # ADJUST FOR HIGHER WORKLOADS
         parallelism-factor = 0.8

         # Max number of threads to cap factor-based parallelism number to
         parallelism-max = 64

         # Setting to "FIFO" to use queue like peeking mode which "poll" or "LIFO" to use stack
         # like peeking mode which "pop".
         task-peeking-mode = "FIFO"
       }
       # Throughput for default Dispatcher, set to 1 for as fair as possible; default 5
       throughput = 5
    }

    timeout = 60000

    allow-java-serializer-usage = false // disable java serialization to ensure kryo is always used
    warn-about-java-serializer-usage = true
    serializers {
      #jackson-json = "akka.serialization.jackson.JacksonJsonSerializer"
      #jackson-cbor = "akka.serialization.jackson.JacksonCborSerializer"
      kryo = "com.twitter.chill.akka.AkkaSerializer"
      #java = "akka.serialization.JavaSerializer"
      #proto = "akka.remote.serialization.ProtobufSerializer"
    }

    serialization-bindings {
      #"tcep.machinenodes.helper.actors.MySerializable" = jackson-json
      "java.io.Serializable" = kryo
    }

    # assign a mailbox type (prio-mailbox) to all actors with the name "TaskManager" and "DistVivaldiRef", even without specifying the mailbox in the Props
    deployment {
      /TaskManager {
        mailbox = prio-mailbox
      }
      /DistVivaldiRef {
        mailbox = prio-mailbox
      }
      /ClientNode {
        mailbox = prio-mailbox
      }
      /knowledge {
        mailbox = prio-mailbox
      }
    }


  }

  remote {
    classic { # classic remoting with netty.tcp, disabled if artery is enabled
        # If this is "on", Akka will log all outbound messages at DEBUG level,
        # if off then they are not logged
        log-sent-messages = on
        log-received-messages = on
        log-frame-size-exceeding = 1b
        log-remote-lifecycle-events = debug

        transport-failure-detector {
              # FQCN of the failure detector implementation.
              # It must implement akka.remote.FailureDetector and have
              # a public constructor with a com.typesafe.config.Config and
              # akka.actor.EventStream parameter.
              implementation-class = "akka.remote.DeadlineFailureDetector"
              # How often keep-alive heartbeat messages should be sent to each connection.
              heartbeat-interval = 1 s
              # Number of potentially lost/delayed heartbeats that will be
              # accepted before considering it to be an anomaly.
              # A margin to the `heartbeat-interval` is important to be able to survive sudden,
              # occasional, pauses in heartbeat arrivals, due to for example garbage collect or
              # network drop.
              acceptable-heartbeat-pause = 5 s # default 120 s
         }

         # After failed to establish an outbound connection, the remoting will mark the
         # address as failed. This configuration option controls how much time should
         # be elapsed before reattempting a new connection. While the address is
         # gated, all messages sent to the address are delivered to dead-letters.
         # Since this setting limits the rate of reconnects setting it to a
         # very short interval (i.e. less than a second) may result in a storm of
         # reconnect attempts.
         retry-gate-closed-for = 500 ms # default 5s

         netty.tcp {
          bind-hostname = "0.0.0.0"
          message-frame-size = 30000000b
          send-buffer-size = 30000000b
          receive-buffer-size = 30000000b
          maximum-frame-size = 30000000b
          connection-timeout = 10 s # default 15s; how long a connect may take until it is timed out

          # LEAVE THIS OFF FOR MININET SIMULATIONS, otherwise heartbeats get lost randomly when handover (publisher movement) happens
          # -> members become marked unreachable (despite being pingable)
          # -> connections to members get gated and all messages to them are sent to deadLetters
          # -> transition messages cannot be delivered until the whole cluster has marked the member as reachable again (messages for this are subject to the same issue!)
          # -> seemingly random failures, chaos ensues
          # Enables TCP Keepalive, subject to the O/S kernelâ€™s configuration
          tcp-keepalive = off # default on;
        }
    }
    artery { # default since akka 2.6.0; uses separate dispatcher for control stream to avoid thread starvation
          # Enable the new remoting with this flag, off is netty.tcp
          enabled = off
          log-received-messages = on
          log-sent-messages = on
          # Hostname to bind a network interface to. Can be set to an ip, hostname
          # or one of the following special values:
          #   "0.0.0.0"            all interfaces
          #   ""                   akka.remote.artery.canonical.hostname
          #   "<getHostAddress>"   InetAddress.getLocalHost.getHostAddress
          #   "<getHostName>"      InetAddress.getLocalHost.getHostName
          bind.hostname = "0.0.0.0"
          advanced.compression.actor-refs.advertisement-interval = 30m # default 1m, set this longer than simulation to avoid countless useless lines when debugging
     }
  }

  cluster {
    seed-nodes = [
        #Names will be resolved by Docker Network. See publish_docker.sh for more details.
        #"akka://tcep@10.0.0.253:"${?constants.base-port}""
        #"akka.tcp://tcep@10.0.0.253:"${?constants.base-port}""
        #"akka.tcp://tcep@20.0.0.15:"${?constants.base-port}""
        "akka.tcp://tcep@simulator:"${?constants.base-port}""
    ]
    retry-unsuccessful-join-after = 5s
    min-nr-of-members = 8 // set by manifest_to_config.py

    # Enable/disable info level logging of cluster events.
    # These are logged with logger name `akka.cluster.Cluster`.
    log-info = on
    # Enable/disable verbose info-level logging of cluster events
    # for temporary troubleshooting. Defaults to 'off'.
    # These are logged with logger name `akka.cluster.Cluster`.
    log-info-verbose = on
    debug {
        # log heartbeat events (very verbose, useful mostly when debugging heartbeating issues)
        verbose-heartbeat-logging = on
        # log verbose details about gossip
        verbose-gossip-logging = on
    }
    # how often should the node send out gossip information?
    gossip-interval = 1 s # default 1s
    # Gossip to random node with newer or older state information, if any with
    # this probability. Otherwise Gossip to any random live node.
    # Probability value is between 0.0 and 1.0. 0.0 means never, 1.0 means always.
    gossip-different-view-probability = 1.0 # default 0.8 -> faster convergence
    # how often should the node move nodes, marked as unreachable by the failure
    # detector, out of the membership ring?
    unreachable-nodes-reaper-interval = 5 s # default 1s
    failure-detector {
      threshold = 12.0 # default 8; higher value -> fewer false positives, longer time until actual failure detection
      acceptable-heartbeat-pause = 60 s # default 10s; leave this high so that mininet handovers do not trigger failure detector (gates connections to "unreachable" (ping on mininet says otherwise) actor until all cluster nodes have marked it as reachable again!)
      heartbeat-interval = 1 s # default 1s
      min-std-deviation = 500 ms # default 100ms
      expected-response-after = 1 s
      # Number of member nodes that each member will send heartbeat messages to,
      # i.e. each node will be monitored by this number of other nodes.
      monitored-by-nr-of-members = 3 # default 5
    }
  }
}
# The id of the dispatcher to use for cluster actors.
# If specified you need to define the settings of the actual dispatcher.
# use separate dispatcher to ensure heartbeats can be sent and received without interference from load on default dispatcher
#akka.cluster.use-dispatcher = cluster-dispatcher
#cluster-dispatcher {
#  type = "Dispatcher"
#  executor = "fork-join-executor"
#  fork-join-executor {
#    parallelism-min = 2
#    parallelism-max = 4
#  }
#}

clustering {
  cluster.name = tcep
}

# Disable legacy metrics in akka-cluster.
akka.cluster.metrics.enabled = off

# Enable metrics extension in akka-cluster-metrics.
#akka.extensions = ["akka.cluster.metrics.ClusterMetricsExtension"]

# Sigar native library extract location during tests.
# Note: use per-jvm-instance folder when running multiple jvm on one host.
akka.cluster.metrics.native-library-extract-folder = ${user.dir}/target/native
